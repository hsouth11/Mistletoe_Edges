---
title: "extra_ord_model_code"
author: "Hanno Southam"
date: "`r Sys.Date()`"
output: html_document
---
Extra code pulled out of ordinal regression script (08_ordinal_regression.Rmd)
when cleaning it up for publication. Contains some useful code on:
(1) Assessing the predictive ability of models (=classification accuracy)
(2) Goodness of fit tests for ordinal models (which don't really work but
are out there in the literature)
(3) Scale effects in the ordinal package


#Checking classification accuracy
Started sketching out this code even though the model selection process above
has an inferential (not predictive) objective. Show what statistics are 
available to test classification accuracy. 

Start by getting predicted probabilities of each DMR level for each 
observation
```{r}
summary(m20)

#Replicate the original data
pred <- modelling %>% 
  select(tree_id, site_id, dmr_f_4, dist_y_h.s, dbh.s, sl_i_tot_rm.s)

#Calculate raw intercation terms
pred <- pred %>%   
  mutate(int_dist_sl = dist_y_h.s*sl_i_tot_rm.s,
         int_dbh_sl = dbh.s*sl_i_tot_rm.s)

#Extract the random effects, add them to the dataframe by site_id
ranef_m20 <- data.frame(ranef = m20$ranef, site_id = unique(modelling$site_id))
pred <- left_join(pred, ranef_m20, by = "site_id")

#Save the model coefficients
coef_m20 <- coefficients(m20)
coef_m20

#Calcualte the logit odds for each level
pred <- pred %>% 
  mutate(lo_0 = coef_m20[1] - coef_m20[3]*dist_y_h.s - 
           coef_m20[4]*dbh.s - coef_m20[5]*sl_i_tot_rm.s -
           coef_m20[6]*int_dist_sl - coef_m20[7]*int_dbh_sl - ranef,
         lo_IBLC2 = coef_m20[2] - coef_m20[3]*dist_y_h.s - 
           coef_m20[4]*dbh.s - coef_m20[5]*sl_i_tot_rm.s -
           coef_m20[6]*int_dist_sl - coef_m20[7]*int_dbh_sl - ranef)

#Take the inverse logit to get cumulative probabilities
pred <- pred %>% 
  mutate(cp_0 = plogis(lo_0),
         cp_IBLC2 = plogis(lo_IBLC2))

#Calculate exact probabilities of each DMR levels from cumulative probabilities
pred <- pred %>% 
  mutate(p_0 = cp_0,
         p_IBLC_2 = cp_IBLC2 - cp_0,
         p_3_5 = 1 - cp_IBLC2)

#Lengthen to show the different probabilities
pred_long <- pred %>% 
  pivot_longer(cols = starts_with("p_"),
               values_to = "prob",
               names_to = "pred_dmr_f_4",
               names_prefix = "p_") %>% 
  mutate(pred_dmr_f_4 = case_match(pred_dmr_f_4,
                          "0" ~ "0",
                          "3_5" ~ "3-5",
                          "IBLC_2" ~ "IBLC-2")) %>% 
  mutate(pred_dmr_f_4 = factor(pred_dmr_f_4, levels = c("0", "IBLC-2", "3-5")))
```

Create with plots with the set up: 
- j-1 panels (j = levels of the response)
- y = predicted probabilities
- x = j levels
Gives you a visual idea of whether each DMR level is well separated by 
predicted probabilities. 
```{r}
ggplot(pred_long, aes(x = pred_dmr_f_4, y = prob)) +
  facet_wrap(~dmr_f_4) +
  geom_point() +
  geom_boxplot(outliers = F,
               fill = NA) +
  theme_classic()
```

Then create classification tables (aka confusion matrices)
```{r}
#Create a function to evaluate which response level has the highest 
#probability, then save that level. 
f_pred_dmr_high <- function(pred_prob_df, dmr_lev) {
  #Apply function to each row to find the column index of the max probability
  max_index <- apply(pred_prob_df, 1, which.max)
  
  # Use the index to select the corresponding level
  pred_dmr <- dmr_lev[max_index]
  
  # Return as a factor with specified levels
  return(factor(pred_dmr, levels = dmr_lev))
}

#Save levels of dmr
dmr_lev <- levels(modelling$dmr_f_4)

#Subset dataframe to just columns that have probabilities of each level
x <- pred %>% 
  select(starts_with("p_"))

#Apply function to get predicted dmr
pred <- pred %>% 
  mutate(pred_dmr_maxp = f_pred_dmr_high(x, dmr_lev))

#Generate a confusion matrix. Used the conf_mat() function in the {yardstick}
#package. 
cm <- conf_mat(pred, truth = dmr_f_4, estimate = pred_dmr_maxp)
cm

#Calculate accuracy (i.e. the proportion of observations that were correctly 
#classified)
accuracy(pred, truth = dmr_f_4, estimate = pred_dmr_maxp)

#Calculate kappa (i.e. the proportion of correct classifications normalized 
#by what you would expect based on chance alone). Weighted kappa is a version 
#of kappa that gives more value to close disagreements (than far ones). You can
#weight it in two ways: (1) quadratic, which weights values as a exponentially
#declining function as you move away from an observation and (2) linear, which
#weights values as a linear declining function as you move away from an 
#observation
kap(pred, truth = dmr_f_4, estimate = pred_dmr_maxp)
kap(pred, truth = dmr_f_4, estimate = pred_dmr_maxp, weighting = "quadratic")
kap(pred, truth = dmr_f_4, estimate = pred_dmr_maxp, weighting = "linear")
```

#Extra code chunks
##Goodness of fit tests
```{r}
#Start by getting predicted values of each response category for the dataset
x <- data.frame(dist_y_h = modelling$dist_y_h, 
                       dbh = modelling$dbh)
x1 <- predict(m11, newdata = x, type = "prob")$fit
predprob <- data.frame("dmr_0" = x1[,1], 
                       "dmr_IBLC_2" = x1[,2], 
                       "dmr_3_5" = x1[,3])

#Plot these
#Notice that the probability of DMR 3-5 is left skewed. Lots of prob = 0. 
#The Lipsitz and Hosmer-Lemeshow test work by grouping data according to their
#predicted probability. The low probability of dmr_3-5 seems to be causing
#problems in Hosmer-Lemeshow test because the expected probability of that 
#class is almost always low. This inflates the test statistic. 
predprob <- predprob %>% 
  pivot_longer(starts_with("dmr"), values_to = "prob", names_to = "dmr") %>% 
  mutate(dmr = factor(dmr, levels = c("dmr_0", "dmr_IBLC_2", "dmr_3_5")))
ggplot(predprob, aes(x = prob)) + geom_histogram() + 
  facet_wrap(~dmr)

#Lipsitz test. 
#Recommend number of groups for this test are 6 <= g <= n/5c, where c = number 
#of levels in the response (3). 
#Increases the number of groups increases the number of parameters in the 
#model that is fit with the score groups. With more than 8 groups, I get
#a warning message. I think this is a problem of data sparseness. 
#Not significant with 6, 7 or 8 groups. 
nrow(modelling)/(5*3) #Max groups are 73
lipsitz.test(m11, g = 7)

#Hosmer-Lemeshow test
#Getting error message. Sparseness issue. 
x <- data.frame(dist_y_h = modelling$dist_y_h, 
                       dbh = modelling$dbh)
fv <- predict(m11, newdata = x, type = "prob")$fit
logitgof(modelling$dmr_f_4, fv, g = 3, ord = TRUE)
```

##Testing different link functions
Conceptually, the one that seems most 
appropriate is the complementary log-log, but others symmetric links (logit, 
probit, etc) could work just fine too.
```{r}
#Model 13
##Predictor: distance from the edge and dbh
##Random effect: site
##Link: complementary log likelihood
##Get a warning message here. 
m13 <- clmm(dmr_f_4 ~ dist_y_h + dbh + (1|site_id), 
           data = modelling,
           link = "cloglog",
           threshold = "flexible")
summary(m13)

#Model 14
##Predictor: distance from the edge and dbh
##Random effect: site
##Link: probit
##Get a warning message here. 
m14 <- clmm(dmr_f_4 ~ dist_y_h + dbh + (1|site_id), 
           data = modelling,
           link = "probit",
           threshold = "flexible")
summary(m14)

#Compare log likelihoods and AIC of the different links
m12$logLik; m13$logLik; m14$logLik #logit and probit provide best fits
AIC(m12); AIC(m13); AIC(m14) #logit and probit provide best fits 
```

##Nominal and scale effects
Test the proportional odds assumption. To do this, we fit models that allow for
nominal effects and then compare it to one without using with a likelihood 
ratio test. Nominal effects haven't been integrates into clmm yet, but they are
in the legacy function clmm2. 
```{r}
#Refit Model 12 (dmr ~ distance + dbh) with clmm2
m15 <- clmm2(dmr_f_4 ~ dist_y_h + dbh, random = site_id, data = modelling,
            link = "logistic", threshold = "flexible", Hess = TRUE)
summary(m15)

#Fit models that allow for nominal effects in distance and dbh

#Model 16
##Predictor: distance from the edge and dbh
##Nominal: distance from the edge
##Random effect: site
##Link: logistic
m16 <- clmm2(dmr_f_4 ~ dbh, nominal = ~dist_y_h, random = site_id, 
             data = modelling, link = "logistic", threshold = "flexible", 
             Hess = TRUE)
summary(m16)

#Model 17
##Predictor: distance from the edge and dbh
##Nominal: distance from the edge
##Random effect: site
##Link: logistic
##Getting error message here
m17 <- clmm2(dmr_f_4 ~ dist_y_h, nominal = ~dbh, random = site_id, 
             data = modelling, link = "logistic", threshold = "flexible", 
             Hess = TRUE)
warnings(m17)

#Run LR ratio tests
#Non-proportional odds significant for distance from the edge but not dbh
anova(m15, m16)
anova(m15, m17)
```

Another option that often produces similar results to nominal effects is 
allowing for scale effects. This allows for the scale parameter of the 
underlying probability density function to change with the predictor. The 
advantage is you only have to fit one additional parameter (as oopose to J-1
with nominal effects) and interpreting the model is easier because the effect
of the predictor is the same across all levels of the response (J). 
```{r}
#Fit scale effect for distance from the edge

#Model 18
##Predictor: distance from the edge and dbh
##Scale: distance from the edge
##Random effect: site
##Link: logistic
##Getting warning that random effects are not converging
m18 <- clmm2(dmr_f_4 ~ dbh, scale = ~dist_y_h, random = site_id, 
             data = modelling, link = "logistic", threshold = "flexible", 
             Hess = TRUE)
warnings(m18)

#Compare this model to the one with nominal effect. Not that the model allowing
#for scale effects is nested in the model with nominal effects. 
#Nominal effects model significantly better
AIC(m16); AIC(m18)
anova(m16, m18)
```

Okay at this point, we have a good working model that is approaching a Hessian
condition that indicates the model is ill-formed. Try adding site level 
variables and see if the model converges. 
```{r}
#Model 19
##Predictor: distance from the edge, dbh, years since harvest
##Nominal: distance from the edge
##Random effect: site
##Link: logistic
##Warning message about random effects
m19 <- clmm2(dmr_f_4 ~ dbh + yr_since_har, nominal = ~dist_y_h, 
             random = site_id, data = modelling, link = "logistic", 
             threshold = "flexible", Hess = TRUE)
summary(m19)
warnings(m19)

#Model 20
##Predictor: distance from the edge, dbh, seed production in mature component
##Nominal: distance from the edge
##Random effect: site
##Link: logistic
##Warning message about random effects
m20 <- clmm2(dmr_f_4 ~ dbh + sp.y, nominal = ~dist_y_h, 
             random = site_id, data = modelling, link = "logistic", 
             threshold = "flexible", Hess = TRUE)
summary(m20)
warnings(m20)

#Model 21
##Predictor: distance from the edge, dbh, BEC zone
##Nominal: distance from the edge
##Random effect: site
##Link: logistic
##Warning message about random effects
m21 <- clmm2(dmr_f_4 ~ dbh + bec, nominal = ~dist_y_h, 
             random = site_id, data = modelling, link = "logistic", 
             threshold = "flexible", Hess = TRUE)
summary(m21)
warnings(m21)
```

##Getting model predictions
Next step is to understand how to get predictions from these models. 
```{r}
#Model 22: dmr ~ dbh.s + distance from edge.s (nominal) + site (random)
summary(m22)

#Model 23: dmr ~ dbh.s + distance from edge.s + site (random)
summary(m23)

#Start by expecting some of the model elements
#Betas coefficients of predictors specified as location (normal, not nominal) 
#effects
m22$beta

#Threshold parameters, which I think are also the nominal coefficients for each
#level logit (j boundary)=
m22$xi

#All the coefficients (location, nominal, thresholds and also one that is used 
#in the link function) 
m22$coefficients

#Random effect estimates. More specifically, the documentations says these are
#the "conditonal modes" of the random effects. There are also estimates of the
#conditional variance for each random effect estimate. 
m22$ranef
m22$condVar

#Fitted values. These are probabilities for each observation in the dataset
#conditional on the random effect. 
#For example, observation 1 is from cr_1, has a dmr of (IBLC-2) and, has a 
#scaled dbh of 1.07 and a scaled distance from the edge of -0.910. The fitted
#value tells you the probability of IBLC-2 at those x values and cr_1 site
modelling %>% select(dmr_f_4, dbh.s, dist_y_h.s, site_id) %>% slice(1)
m22$fitted.values %>% head()

#To get predicted values for the average site (no random effect) you use the 
#predict function.
#Note: if you just run predict(model), you get the conditional predicted
#probabilities
predict(m22, newdata = modelling) %>% head()

#A few other things in the model object
#True or false indicating if the model converged
m22$convergence

#Vector of the response variable in the original dataframe
m22$y

#Levels of the response variable
m22$lev
```

Check understanding by calculating the fitted value of the first 
observation by hand. The model is: 
$\text{dmr_f_4} \sim \text{dbh.s} + \text{(1 | site_id)} \quad \text{with nominal effect:} \quad \sim \text{dist_y_h.s}$

```{r}
#Extract first observation
obs1 <- modelling %>% select(dmr_f_4, dbh.s, dist_y_h.s, site_id) %>% 
  slice(1)
obs1

#Save dbh and distance values as simple numerics
obs1.num <- obs1 %>% select(dbh.s, dist_y_h.s) %>% unlist()

#Compare the what the fitted value for the first observation was
m22$fitted.values %>% head(1)

#Model summary
summary(m22)

#Calculate the logit(odds) (= underlying latent variable) for P(Y<=0) and 
#P(Y<=IBLC-2). 
m22$coefficients
lgit1 <- (m22$coefficients[1] + m22$coefficients[3]*obs1.num[2]) - 
  m22$coefficients[5]*obs1.num[1] - m22$ranef[1]
lgit2 <- (m22$coefficients[2] + m22$coefficients[4]*obs1.num[2]) - 
  m22$coefficients[5]*obs1.num[1] - m22$ranef[1]

#These are on the log odds scale. Convert these to the estimated cumulative 
#probability by taking the inverse of the logit, which the plogis() function
#does
plogis(lgit1) #P(Y<=0)
plogis(lgit2) #P(Y<=IBLC-2)

#Subtract the cumulative probabilities to get P(Y = IBLC-2) and compare with
#the fitted value (should be identical)
p_iblc2 <- plogis(lgit2) - plogis(lgit1)
p_iblc2
m22$fitted.values %>% head(1)
```

The fitted values in the model object only provide the estimated probability 
for the observed DMR value of each observation. To assess the classification
accuracy, we need estimates of each DMR level for each observation and then we
can compare it to what the observed DMR was. 
```{r}
#Replicate the original data
pred_re_m22 <- data.frame(tree_id = modelling$tree_id,
                dmr_f_4 = factor(modelling$dmr_f_4),
                dist_y_h.s = modelling$dist_y_h.s,
                dbh.s = modelling$dbh.s,
                site_id = modelling$site_id)

#Add fitted proabilities to the dataframe so we can cross check calculations
pred_re_m22 <- pred_re_m22 %>% 
  mutate(fit_m22 = fitted(m22))

#Save the model coefficients
coef_m22 <- coefficients(m22)
coef_m22

#Epred_re_m22tract random effects, add them to the dataframe by site_id
ranef_m22 <- data.frame(ranef = m22$ranef, site_id = unique(modelling$site_id))
pred_re_m22 <- left_join(pred_re_m22, ranef_m22, by = "site_id")

#Calcualte cumulative probabilities: P(DMR<=0) and P(DMR<= IBLC-2)
summary(m22)
pred_re_m22 <- pred_re_m22 %>% 
  mutate(cp_0 = plogis((coef_m22[1] + coef_m22[3]*dist_y_h.s) - 
           coef_m22[5]*dbh.s - ranef),
         cp_IBLC2 = plogis((coef_m22[2] + coef_m22[4]*dist_y_h.s) - 
           coef_m22[5]*dbh.s - ranef)
         )

#Calculate epred_re_m22act probabilities of each level
pred_re_m22 <- pred_re_m22 %>% 
  mutate(prob_0 = cp_0,
         prob_IBLC_2 = cp_IBLC2 - cp_0,
         prob_3_5 = 1- cp_IBLC2)
```

##Testing classification accuracy
Plot the predicted probabilities of each level to see if there is good 
separation. i.e. for a tree that has an observed dmr of IBLC-2, is the 
probability of that level higher (and separated) from the probabilities of the
other levels?
```{r}
#Gather columns of predicted probabilities
x <- pred_re_m22 %>% 
  pivot_longer(cols = starts_with("prob"),
               values_to = "prob",
               names_to = "pred_dmr",
               names_prefix = "prob_") %>% 
  mutate(pred_dmr = factor(pred_dmr, levels = c("0", "IBLC_2", "3_5")))

#Plot
ggplot(x, aes(x = prob, y = pred_dmr)) + 
  geom_jitter() + 
  geom_boxplot(outliers = FALSE, fill = NA) +
  facet_wrap(~dmr_f_4)
```

Calculate kappa 
```{r}
#First step is to generate a confusion matrix. 

#Create a function to evaluate which response level has the highest 
#probability, then save that level. 
f_pred_dmr_high <- function(pred_prob_df, dmr_lev) {
  #Apply function to each row to find the column index of the max probability
  max_index <- apply(pred_prob_df, 1, which.max)
  
  # Use the index to select the corresponding level
  pred_dmr <- dmr_lev[max_index]
  
  # Return as a factor with specified levels
  return(factor(pred_dmr, levels = dmr_lev))
}

#Save levels of dmr
dmr_lev <- levels(modelling$dmr_f_4)

#Subset dataframe to just columns that have probabilities of each level
x <- pred_re_m22 %>% 
  select(starts_with("prob"))

#Apply function to get predicted dmr
pred_re_m22 <- pred_re_m22 %>% 
  mutate(pred_dmr = f_pred_dmr_high(x, dmr_lev))

#Generate a confusion matrix. Used the conf_mat() function in the {yardstick}
#package. 
cm_m22 <- conf_mat(pred_re_m22, truth = dmr_f_4, estimate = pred_dmr)
cm_m22

#Calculate accuracy (i.e. the proportion of observations that were correctly 
#classified)
accuracy(pred_re_m22, truth = dmr_f_4, estimate = pred_dmr)

#Calculate kappa (i.e. the proportion of correct classifications normalized 
#by what you would expect based on chance alone). Weighted kappa is a version 
#of kappa that gives more value to close disagreements (than far ones). You can
#weight it in two ways: (1) quadratic, which weights values as a exponentially
#declining function as you move away from an observation and (2) linear, which
#weights values as a linear declining function as you move away from an 
#observation
kap(pred_re_m22, truth = dmr_f_4, estimate = pred_dmr)
kap(pred_re_m22, truth = dmr_f_4, estimate = pred_dmr, weighting = "quadratic")
kap(pred_re_m22, truth = dmr_f_4, estimate = pred_dmr, weighting = "linear")
```

Assigning a DMR based on the response level with the probability is arbitrary. 
Instead, assign using a weighted dice roll. The "dice" has the same number of 
sides as there are levels in the response. The probability of each side, is the 
probability of that level in the response. Here, we do one dice roll for each 
observation and get a DMR value. This is what I am imagining we will do when we
extend this to the stand scale predictive model, so really what we are
interested in is if this produces similar levels of infection at a site level;
predicting individual trees isn't as important as predicting % infection at the 
site. 
```{r}
#With help from ChatGPT
#Function to assign a response level using weighted probabilities
f_pred_dmr_wt <- function(pred_prob_df, dmr_lev) {
  # Apply function to each row to sample a level with weighted probabilities
  pred_dmr <- apply(pred_prob_df, 1, function(probabilities) {
    # Use sample() to randomly select a level with probabilities as weights
    sample(dmr_lev, size = 1, prob = probabilities)
  })
  
  # Return as a factor with specified levels
  return(factor(pred_dmr, levels = dmr_lev))
}

# Subset dataframe to just columns that have probabilities of each level
x <- pred_re_m22 %>%
  select(starts_with("prob"))

# Apply the new function to get predicted dmr using weighted sampling
pred_re_m22 <- pred_re_m22 %>%
  mutate(pred_dmr_wt = f_pred_dmr_wt(x, dmr_lev))
```

Compare the proportions of infected trees in each DMR response level based on
assigning a DMR with the highest probability and with a weighted dice roll.
```{r}
#Check the levels in the observed and predicted dmrs are the same
levels(pred_re_m22$dmr_f_4)
levels(pred_re_m22$pred_dmr)
levels(pred_re_m22$pred_dmr_wt)

#Subset the dataframe
x <- pred_re_m22 %>% 
  select(site_id, dmr_f_4, pred_dmr, pred_dmr_wt)

#Gather the columns
x <- x %>% 
  pivot_longer(cols = c("dmr_f_4", "pred_dmr", "pred_dmr_wt"),
               values_to = "dmr_val",
               names_to = "dmr_ver")

#Get the total number of trees for each site
x1 <- modelling %>% 
  group_by(site_id) %>% 
  summarise(tot_trees = n()) 

#Summarise the number of trees in each dmr class by each dmr version (observed
#and predicted by highest prob or dice roll) within each site 
x <- x %>% 
  group_by(site_id, dmr_ver, dmr_val) %>% 
  summarise(n_trees = n(), .groups = "drop") %>% 
  complete(site_id, dmr_ver, dmr_val, fill = list(n_trees = 0))

#Add the total number of trees at each site and convert to proportion
x <- left_join(x, x1, by = "site_id")
x <- x %>% 
  mutate(p_trees = n_trees/tot_trees)

#Graph the proportion of each trees in each dmr class for each site
ggplot(x, aes(x = site_id, y = p_trees, colour = dmr_ver)) + 
  geom_jitter(width = 0.2) + 
  facet_wrap(~dmr_val)

#Compare the accuracy of the two dmr assignment methods
x2 <- x %>% select(-c(n_trees, tot_trees)) %>% 
  group_by(site_id, dmr_val) %>% 
  mutate(id = row_number()) %>% 
  pivot_wider(names_from = dmr_ver,
              values_from = p_trees)
```
